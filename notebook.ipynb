{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf603d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/Robotic/T5.2/ob2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35156b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolact'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 2936, done.\u001b[K\n",
      "remote: Total 2936 (delta 0), reused 0 (delta 0), pack-reused 2936 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2936/2936), 21.20 MiB | 18.23 MiB/s, done.\n",
      "Resolving deltas: 100% (2002/2002), done.\n",
      "/home/user/Downloads/Robotic/T5.2/ob2/yolact\n"
     ]
    }
   ],
   "source": [
    "# Clone the official YOLACT repository\n",
    "!git clone https://github.com/dbolya/yolact.git\n",
    "%cd yolact\n",
    "\n",
    "# # Install required packages\n",
    "# !pip install torch torchvision opencv-python Pillow pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137a3c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ CUDA not available, running on CPU.\n",
      "YOLACT model and helper classes defined.\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the full, CPU-safe model definition from your yolact_cpu.py file.\n",
    "# By placing this here, we make the notebook self-contained and avoid the import error.\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import Bottleneck\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from math import sqrt\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from data.config import cfg, mask_type\n",
    "from layers import Detect\n",
    "from layers.interpolate import InterpolateModule\n",
    "from backbone import construct_backbone\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import timer\n",
    "from utils.functions import MovingAverage, make_net\n",
    "from layers.modules import MultiBoxLoss\n",
    "import os\n",
    "\n",
    "# This is the corrected CUDA initialization from your yolact_cpu.py\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "else:\n",
    "    print(\"⚠️ CUDA not available, running on CPU.\")\n",
    "\n",
    "# --- The full, corrected code from yolact_cpu.py starts here ---\n",
    "use_jit = torch.cuda.device_count() <= 1\n",
    "if not use_jit:\n",
    "    print('Multiple GPUs detected! Turning off JIT.')\n",
    "\n",
    "ScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module\n",
    "script_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def __init__(self, nets, extra_params):\n",
    "        super().__init__()\n",
    "        self.nets = nn.ModuleList(nets)\n",
    "        self.extra_params = extra_params\n",
    "    def forward(self, x):\n",
    "        return torch.cat([net(x) for net in self.nets], dim=1, **self.extra_params)\n",
    "\n",
    "prior_cache = defaultdict(lambda: None)\n",
    "\n",
    "class PredictionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1024, aspect_ratios=[[1]], scales=[1], parent=None, index=0):\n",
    "        super().__init__()\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.mask_dim    = cfg.mask_dim\n",
    "        self.num_priors  = sum(len(x)*len(scales) for x in aspect_ratios)\n",
    "        self.parent      = [parent]\n",
    "        self.index       = index\n",
    "        self.num_heads   = cfg.num_heads\n",
    "        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n",
    "            self.mask_dim = self.mask_dim // self.num_heads\n",
    "        if cfg.mask_proto_prototypes_as_features:\n",
    "            in_channels += self.mask_dim\n",
    "        if parent is None:\n",
    "            if cfg.extra_head_net is None:\n",
    "                out_channels = in_channels\n",
    "            else:\n",
    "                self.upfeature, out_channels = make_net(in_channels, cfg.extra_head_net)\n",
    "            if cfg.use_prediction_module:\n",
    "                self.block = Bottleneck(out_channels, out_channels // 4)\n",
    "                self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)\n",
    "                self.bn = nn.BatchNorm2d(out_channels)\n",
    "            self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4,                **cfg.head_layer_params)\n",
    "            self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, **cfg.head_layer_params)\n",
    "            self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim,    **cfg.head_layer_params)\n",
    "            if cfg.use_mask_scoring:\n",
    "                self.score_layer = nn.Conv2d(out_channels, self.num_priors, **cfg.head_layer_params)\n",
    "            if cfg.use_instance_coeff:\n",
    "                self.inst_layer = nn.Conv2d(out_channels, self.num_priors * cfg.num_instance_coeffs, **cfg.head_layer_params)\n",
    "            def make_extra(num_layers):\n",
    "                if num_layers == 0: return lambda x: x\n",
    "                else:\n",
    "                    return nn.Sequential(*sum([[\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(inplace=True)\n",
    "                    ] for _ in range(num_layers)], []))\n",
    "            self.bbox_extra, self.conf_extra, self.mask_extra = [make_extra(x) for x in cfg.extra_layers]\n",
    "            if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_coeff_gate:\n",
    "                self.gate_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim, kernel_size=3, padding=1)\n",
    "        self.aspect_ratios, self.scales = aspect_ratios, scales\n",
    "        self.priors, self.last_conv_size, self.last_img_size = None, None, None\n",
    "    def forward(self, x):\n",
    "        src = self if self.parent[0] is None else self.parent[0]\n",
    "        conv_h, conv_w = x.size(2), x.size(3)\n",
    "        if cfg.extra_head_net is not None: x = src.upfeature(x)\n",
    "        if cfg.use_prediction_module:\n",
    "            a = src.block(x)\n",
    "            b = F.relu(src.bn(src.conv(x)))\n",
    "            x = a + b\n",
    "        bbox_x, conf_x, mask_x = src.bbox_extra(x), src.conf_extra(x), src.mask_extra(x)\n",
    "        bbox = src.bbox_layer(bbox_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
    "        conf = src.conf_layer(conf_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
    "        if cfg.eval_mask_branch:\n",
    "            mask = src.mask_layer(mask_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n",
    "        else:\n",
    "            mask = torch.zeros(x.size(0), bbox.size(1), self.mask_dim, device=bbox.device)\n",
    "        if cfg.use_mask_scoring:\n",
    "            score = src.score_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 1)\n",
    "        if cfg.use_instance_coeff:\n",
    "            inst = src.inst_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, cfg.num_instance_coeffs)\n",
    "        if cfg.use_yolo_regressors:\n",
    "            bbox[:, :, :2] = torch.sigmoid(bbox[:, :, :2]) - 0.5\n",
    "            bbox[:, :, 0] /= conv_w\n",
    "            bbox[:, :, 1] /= conv_h\n",
    "        if cfg.eval_mask_branch:\n",
    "            if cfg.mask_type == mask_type.direct: mask = torch.sigmoid(mask)\n",
    "            elif cfg.mask_type == mask_type.lincomb:\n",
    "                mask = cfg.mask_proto_coeff_activation(mask)\n",
    "                if cfg.mask_proto_coeff_gate:\n",
    "                    gate = src.gate_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n",
    "                    mask = mask * torch.sigmoid(gate)\n",
    "        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n",
    "            mask = F.pad(mask, (self.index * self.mask_dim, (self.num_heads - self.index - 1) * self.mask_dim), mode='constant', value=0)\n",
    "        priors = self.make_priors(conv_h, conv_w, x.device)\n",
    "        preds = { 'loc': bbox, 'conf': conf, 'mask': mask, 'priors': priors }\n",
    "        if cfg.use_mask_scoring: preds['score'] = score\n",
    "        if cfg.use_instance_coeff: preds['inst'] = inst\n",
    "        return preds\n",
    "    def make_priors(self, conv_h, conv_w, device):\n",
    "        global prior_cache\n",
    "        size = (conv_h, conv_w)\n",
    "        if self.last_img_size != (cfg._tmp_img_w, cfg._tmp_img_h):\n",
    "            prior_data = []\n",
    "            for j, i in product(range(conv_h), range(conv_w)):\n",
    "                x, y = (i + 0.5) / conv_w, (j + 0.5) / conv_h\n",
    "                for ars in self.aspect_ratios:\n",
    "                    for scale in self.scales:\n",
    "                        for ar in ars:\n",
    "                            if not cfg.backbone.preapply_sqrt: ar = sqrt(ar)\n",
    "                            if cfg.backbone.use_pixel_scales:\n",
    "                                w, h = scale * ar / cfg.max_size, scale / ar / cfg.max_size\n",
    "                            else:\n",
    "                                w, h = scale * ar / conv_w, scale / ar / conv_h\n",
    "                            if cfg.backbone.use_square_anchors: h = w\n",
    "                            prior_data += [x, y, w, h]\n",
    "            self.priors = torch.Tensor(prior_data, device=device).view(-1, 4).detach()\n",
    "            self.priors.requires_grad = False\n",
    "            self.last_img_size = (cfg._tmp_img_w, cfg._tmp_img_h)\n",
    "            self.last_conv_size = (conv_w, conv_h)\n",
    "            prior_cache[size] = None\n",
    "        elif self.priors.device != device:\n",
    "            if prior_cache[size] is None: prior_cache[size] = {}\n",
    "            if device not in prior_cache[size]:\n",
    "                prior_cache[size][device] = self.priors.to(device)\n",
    "            self.priors = prior_cache[size][device]\n",
    "        return self.priors\n",
    "\n",
    "class FPN(ScriptModuleWrapper):\n",
    "    __constants__ = ['interpolation_mode', 'num_downsample', 'use_conv_downsample', 'relu_pred_layers', 'lat_layers', 'pred_layers', 'downsample_layers', 'relu_downsample_layers']\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.lat_layers  = nn.ModuleList([nn.Conv2d(x, cfg.fpn.num_features, kernel_size=1) for x in reversed(in_channels)])\n",
    "        padding = 1 if cfg.fpn.pad else 0\n",
    "        self.pred_layers = nn.ModuleList([nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=padding) for _ in in_channels])\n",
    "        if cfg.fpn.use_conv_downsample:\n",
    "            self.downsample_layers = nn.ModuleList([nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=1, stride=2) for _ in range(cfg.fpn.num_downsample)])\n",
    "        self.interpolation_mode, self.num_downsample, self.use_conv_downsample, self.relu_downsample_layers, self.relu_pred_layers = cfg.fpn.interpolation_mode, cfg.fpn.num_downsample, cfg.fpn.use_conv_downsample, cfg.fpn.relu_downsample_layers, cfg.fpn.relu_pred_layers\n",
    "    @script_method_wrapper\n",
    "    def forward(self, convouts:List[torch.Tensor]):\n",
    "        out, x = [], torch.zeros(1, device=convouts[0].device)\n",
    "        for i in range(len(convouts)): out.append(x)\n",
    "        j = len(convouts)\n",
    "        for lat_layer in self.lat_layers:\n",
    "            j -= 1\n",
    "            if j < len(convouts) - 1:\n",
    "                _, _, h, w = convouts[j].size()\n",
    "                x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=False)\n",
    "            x = x + lat_layer(convouts[j])\n",
    "            out[j] = x\n",
    "        j = len(convouts)\n",
    "        for pred_layer in self.pred_layers:\n",
    "            j -= 1\n",
    "            out[j] = pred_layer(out[j])\n",
    "            if self.relu_pred_layers: F.relu(out[j], inplace=True)\n",
    "        cur_idx = len(out)\n",
    "        if self.use_conv_downsample:\n",
    "            for downsample_layer in self.downsample_layers: out.append(downsample_layer(out[-1]))\n",
    "        else:\n",
    "            for idx in range(self.num_downsample): out.append(nn.functional.max_pool2d(out[-1], 1, stride=2))\n",
    "        if self.relu_downsample_layers:\n",
    "            for idx in range(len(out) - cur_idx): out[idx] = F.relu(out[idx + cur_idx], inplace=False)\n",
    "        return out\n",
    "\n",
    "class FastMaskIoUNet(ScriptModuleWrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_channels = 1\n",
    "        last_layer = [(cfg.num_classes-1, 1, {})]\n",
    "        self.maskiou_net, _ = make_net(input_channels, cfg.maskiou_net + last_layer, include_last_relu=True)\n",
    "    def forward(self, x):\n",
    "        x = self.maskiou_net(x)\n",
    "        return F.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)\n",
    "\n",
    "class Yolact(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = construct_backbone(cfg.backbone)\n",
    "        if cfg.freeze_bn: self.freeze_bn()\n",
    "        if cfg.mask_type == mask_type.direct: cfg.mask_dim = cfg.mask_size**2\n",
    "        elif cfg.mask_type == mask_type.lincomb:\n",
    "            if cfg.mask_proto_use_grid:\n",
    "                self.grid = torch.Tensor(np.load(cfg.mask_proto_grid_file))\n",
    "                self.num_grids = self.grid.size(0)\n",
    "            else: self.num_grids = 0\n",
    "            self.proto_src = cfg.mask_proto_src\n",
    "            if self.proto_src is None: in_channels = 3\n",
    "            elif cfg.fpn is not None: in_channels = cfg.fpn.num_features\n",
    "            else: in_channels = self.backbone.channels[self.proto_src]\n",
    "            in_channels += self.num_grids\n",
    "            self.proto_net, cfg.mask_dim = make_net(in_channels, cfg.mask_proto_net, include_last_relu=False)\n",
    "            if cfg.mask_proto_bias: cfg.mask_dim += 1\n",
    "        self.selected_layers = cfg.backbone.selected_layers\n",
    "        src_channels = self.backbone.channels\n",
    "        if cfg.use_maskiou: self.maskiou_net = FastMaskIoUNet()\n",
    "        if cfg.fpn is not None:\n",
    "            self.fpn = FPN([src_channels[i] for i in self.selected_layers])\n",
    "            self.selected_layers = list(range(len(self.selected_layers) + cfg.fpn.num_downsample))\n",
    "            src_channels = [cfg.fpn.num_features] * len(self.selected_layers)\n",
    "        self.prediction_layers = nn.ModuleList()\n",
    "        cfg.num_heads = len(self.selected_layers)\n",
    "        for idx, layer_idx in enumerate(self.selected_layers):\n",
    "            parent = self.prediction_layers[0] if cfg.share_prediction_module and idx > 0 else None\n",
    "            pred = PredictionModule(src_channels[layer_idx], src_channels[layer_idx],\n",
    "                                    aspect_ratios = cfg.backbone.pred_aspect_ratios[idx],\n",
    "                                    scales        = cfg.backbone.pred_scales[idx],\n",
    "                                    parent=parent, index=idx)\n",
    "            self.prediction_layers.append(pred)\n",
    "        if cfg.use_class_existence_loss:\n",
    "            self.class_existence_fc = nn.Linear(src_channels[-1], cfg.num_classes - 1)\n",
    "        if cfg.use_semantic_segmentation_loss:\n",
    "            self.semantic_seg_conv = nn.Conv2d(src_channels[0], cfg.num_classes-1, kernel_size=1)\n",
    "        self.detect = Detect(cfg.num_classes, bkg_label=0, top_k=cfg.nms_top_k, conf_thresh=cfg.nms_conf_thresh, nms_thresh=cfg.nms_thresh)\n",
    "    def save_weights(self, path): torch.save(self.state_dict(), path)\n",
    "    def load_weights(self, path):\n",
    "        state_dict = torch.load(path, map_location=torch.device('cpu')) # Added map_location for CPU\n",
    "        for key in list(state_dict.keys()):\n",
    "            if key.startswith('backbone.layer') and not key.startswith('backbone.layers'):\n",
    "                del state_dict[key]\n",
    "            if key.startswith('fpn.downsample_layers.'):\n",
    "                if cfg.fpn is not None and int(key.split('.')[2]) >= cfg.fpn.num_downsample:\n",
    "                    del state_dict[key]\n",
    "        self.load_state_dict(state_dict)\n",
    "    def init_weights(self, backbone_path):\n",
    "        self.backbone.init_backbone(backbone_path)\n",
    "        conv_constants = getattr(nn.Conv2d(1, 1, 1), '__constants__')\n",
    "        def all_in(x, y):\n",
    "            for _x in x:\n",
    "                if _x not in y: return False\n",
    "            return True\n",
    "        for name, module in self.named_modules():\n",
    "            is_script_conv = False\n",
    "            if 'Script' in type(module).__name__:\n",
    "                if hasattr(module, 'original_name'):\n",
    "                    is_script_conv = 'Conv' in module.original_name\n",
    "                else:\n",
    "                    is_script_conv = (all_in(module.__dict__['_constants_set'], conv_constants) and all_in(conv_constants, module.__dict__['_constants_set']))\n",
    "            is_conv_layer = isinstance(module, nn.Conv2d) or is_script_conv\n",
    "            if is_conv_layer and module not in self.backbone.backbone_modules:\n",
    "                nn.init.xavier_uniform_(module.weight.data)\n",
    "                if module.bias is not None:\n",
    "                    if cfg.use_focal_loss and 'conf_layer' in name:\n",
    "                        if not cfg.use_sigmoid_focal_loss:\n",
    "                            module.bias.data[0]  = np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n",
    "                            module.bias.data[1:] = -np.log(module.bias.size(0) - 1)\n",
    "                        else:\n",
    "                            module.bias.data[0]  = -np.log(cfg.focal_loss_init_pi / (1 - cfg.focal_loss_init_pi))\n",
    "                            module.bias.data[1:] = -np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n",
    "                    else:\n",
    "                        module.bias.data.zero_()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if cfg.freeze_bn: self.freeze_bn()\n",
    "    def freeze_bn(self, enable=False):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.train() if enable else module.eval()\n",
    "                module.weight.requires_grad = enable\n",
    "                module.bias.requires_grad = enable\n",
    "    def forward(self, x):\n",
    "        _, _, img_h, img_w = x.size()\n",
    "        cfg._tmp_img_h, cfg._tmp_img_w = img_h, img_w\n",
    "        with timer.env('backbone'): outs = self.backbone(x)\n",
    "        if cfg.fpn is not None:\n",
    "            with timer.env('fpn'):\n",
    "                outs = [outs[i] for i in cfg.backbone.selected_layers]\n",
    "                outs = self.fpn(outs)\n",
    "        proto_out = None\n",
    "        if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:\n",
    "            with timer.env('proto'):\n",
    "                proto_x = x if self.proto_src is None else outs[self.proto_src]\n",
    "                if self.num_grids > 0:\n",
    "                    grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)\n",
    "                    proto_x = torch.cat([proto_x, grids], dim=1)\n",
    "                proto_out = self.proto_net(proto_x)\n",
    "                proto_out = cfg.mask_proto_prototype_activation(proto_out)\n",
    "                if cfg.mask_proto_prototypes_as_features:\n",
    "                    proto_downsampled = proto_out.clone()\n",
    "                    if cfg.mask_proto_prototypes_as_features_no_grad:\n",
    "                        proto_downsampled = proto_out.detach()\n",
    "                proto_out = proto_out.permute(0, 2, 3, 1).contiguous()\n",
    "                if cfg.mask_proto_bias:\n",
    "                    bias_shape = [x for x in proto_out.size()]\n",
    "                    bias_shape[-1] = 1\n",
    "                    proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)\n",
    "        with timer.env('pred_heads'):\n",
    "            pred_outs = { 'loc': [], 'conf': [], 'mask': [], 'priors': [] }\n",
    "            if cfg.use_mask_scoring: pred_outs['score'] = []\n",
    "            if cfg.use_instance_coeff: pred_outs['inst'] = []\n",
    "            for idx, pred_layer in zip(self.selected_layers, self.prediction_layers):\n",
    "                pred_x = outs[idx]\n",
    "                if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_prototypes_as_features:\n",
    "                    proto_downsampled = F.interpolate(proto_downsampled, size=outs[idx].size()[2:], mode='bilinear', align_corners=False)\n",
    "                    pred_x = torch.cat([pred_x, proto_downsampled], dim=1)\n",
    "                if cfg.share_prediction_module and pred_layer is not self.prediction_layers[0]:\n",
    "                    pred_layer.parent = [self.prediction_layers[0]]\n",
    "                p = pred_layer(pred_x)\n",
    "                for k, v in p.items(): pred_outs[k].append(v)\n",
    "        for k, v in pred_outs.items():\n",
    "            pred_outs[k] = torch.cat(v, -2)\n",
    "        if proto_out is not None: pred_outs['proto'] = proto_out\n",
    "        if self.training:\n",
    "            if cfg.use_class_existence_loss:\n",
    "                pred_outs['classes'] = self.class_existence_fc(outs[-1].mean(dim=(2, 3)))\n",
    "            if cfg.use_semantic_segmentation_loss:\n",
    "                pred_outs['segm'] = self.semantic_seg_conv(outs[0])\n",
    "            return pred_outs\n",
    "        else:\n",
    "            if cfg.use_mask_scoring:\n",
    "                pred_outs['score'] = torch.sigmoid(pred_outs['score'])\n",
    "            if cfg.use_focal_loss:\n",
    "                if cfg.use_sigmoid_focal_loss:\n",
    "                    pred_outs['conf'] = torch.sigmoid(pred_outs['conf'])\n",
    "                    if cfg.use_mask_scoring: pred_outs['conf'] *= pred_outs['score']\n",
    "                elif cfg.use_objectness_score:\n",
    "                    objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])\n",
    "                    pred_outs['conf'][:, :, 1:] = objectness[:, :, None] * F.softmax(pred_outs['conf'][:, :, 1:], -1)\n",
    "                    pred_outs['conf'][:, :, 0 ] = 1 - objectness\n",
    "                else:\n",
    "                    pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)\n",
    "            else:\n",
    "                if cfg.use_objectness_score:\n",
    "                    objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])\n",
    "                    pred_outs['conf'][:, :, 1:] = (objectness > 0.10)[..., None] * F.softmax(pred_outs['conf'][:, :, 1:], dim=-1)\n",
    "                else:\n",
    "                    pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)\n",
    "            return self.detect(pred_outs, self)\n",
    "\n",
    "# --- Training Helper Class ---\n",
    "class NetLoss(nn.Module):\n",
    "    def __init__(self, net:Yolact, criterion:MultiBoxLoss):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def forward(self, images, targets, masks, num_crowds):\n",
    "        preds = self.net(images)\n",
    "        losses = self.criterion(self.net, preds, targets, masks, num_crowds)\n",
    "        return losses\n",
    "\n",
    "# A simple class to mimic the command-line arguments\n",
    "class Args:\n",
    "    def __init__(self, use_cuda):\n",
    "        self.batch_size = 4\n",
    "        self.resume = None\n",
    "        self.start_iter = -1\n",
    "        self.num_workers = 0\n",
    "        self.cuda = use_cuda\n",
    "        self.lr = 1e-3\n",
    "        self.momentum = 0.9\n",
    "        self.decay = 5e-4\n",
    "        self.gamma = 0.1\n",
    "        self.save_folder = 'weights/'\n",
    "        self.log_folder = 'logs/'\n",
    "        self.config = None\n",
    "        self.save_interval = 2000\n",
    "        self.validation_epoch = -1 # Disable validation for simplicity in this example\n",
    "        self.keep_latest = True\n",
    "        self.log = True\n",
    "\n",
    "print(\"YOLACT model and helper classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a679bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-05 10:47:11--  https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 13.224.181.117, 13.224.181.60, 13.224.181.40, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|13.224.181.117|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102502400 (98M) [application/octet-stream]\n",
      "Saving to: ‘weights/resnet50-19c8e357.pth’\n",
      "\n",
      "resnet50-19c8e357.p 100%[===================>]  97.75M   108MB/s    in 0.9s    \n",
      "\n",
      "2025-09-05 10:47:12 (108 MB/s) - ‘weights/resnet50-19c8e357.pth’ saved [102502400/102502400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the official pre-trained weights for ResNet-50\n",
    "!wget https://download.pytorch.org/models/resnet50-19c8e357.pth -P weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c4a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying patch to ResNetBackbone for PyTorch 2.6+ compatibility...\n",
      "Patch applied successfully.\n"
     ]
    }
   ],
   "source": [
    "# This cell patches the ResNetBackbone to be compatible with newer PyTorch versions.\n",
    "\n",
    "from backbone import ResNetBackbone\n",
    "import torch\n",
    "\n",
    "print(\"Applying patch to ResNetBackbone for PyTorch 2.6+ compatibility...\")\n",
    "\n",
    "# Keep a copy of the original function, just in case (optional, but good practice)\n",
    "original_init_backbone = ResNetBackbone.init_backbone\n",
    "\n",
    "# Define our new, corrected function that includes `weights_only=False`\n",
    "def fixed_init_backbone(self, path):\n",
    "    \"\"\" Initializes the backbone weights for training. \"\"\"\n",
    "    # THE FIX IS HERE: Add weights_only=False to the torch.load call\n",
    "    state_dict = torch.load(path, weights_only=False)\n",
    "\n",
    "    # The rest of this function is copied from the original backbone.py\n",
    "    # to ensure the weight keys are correctly renamed for the model.\n",
    "    keys = list(state_dict)\n",
    "    for key in keys:\n",
    "        if key.startswith('layer'):\n",
    "            idx = int(key[5])\n",
    "            new_key = 'layers.' + str(idx-1) + key[6:]\n",
    "            state_dict[new_key] = state_dict.pop(key)\n",
    "    \n",
    "    # The official resnet weights are trained in BGR, so we need to convert them to RGB\n",
    "    # Also, for some stupid reason, the conv1 layer is named something else\n",
    "    if 'conv1.0.weight' in state_dict:\n",
    "        state_dict['conv1.weight'] = state_dict.pop('conv1.0.weight')\n",
    "    # THE FIX IS HERE: Call load_state_dict on `self` directly, not `self.model`\n",
    "    self.load_state_dict(state_dict, strict=False)\n",
    "    print('Backbone weights loaded successfully.')\n",
    "\n",
    "# Apply the patch: Overwrite the original method with our fixed version\n",
    "ResNetBackbone.init_backbone = fixed_init_backbone\n",
    "\n",
    "print(\"Patch applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad45bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration is set for ResNet-50 backbone.\n",
      "Number of classes: 7\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "from data.config import Config, yolact_base_config, resnet50_backbone, mask_type, fpn_base\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "# Set this to True to use a GPU, or False to use the CPU.\n",
    "USE_CUDA = False\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Define your custom dataset\n",
    "dataset_base = Config({\n",
    "    'name': 'My Dataset',\n",
    "    'train_images': '../my_dataset/train',\n",
    "    'train_info':   '../my_dataset/train/_annotations.coco.json',\n",
    "    'valid_images': '../my_dataset/valid',\n",
    "    'valid_info':   '../my_dataset/valid/_annotations.coco.json',\n",
    "    'class_names': ('bench', 'chair', 'couch', 'dining table', 'laptop', 'person'),\n",
    "    'has_gt': True,\n",
    "    'label_map': None\n",
    "})\n",
    "\n",
    "# Create your final config, telling it to use the pre-defined resnet50_backbone\n",
    "yolact_my_config = yolact_base_config.copy({\n",
    "    'name': 'yolact_resnet50_custom', # Changed name to reflect the new backbone\n",
    "    \n",
    "    # Use our custom dataset\n",
    "    'dataset': dataset_base,\n",
    "    \n",
    "    # FIX: Use the imported resnet50_backbone configuration\n",
    "    'backbone': resnet50_backbone.copy({\n",
    "        'selected_layers': list(range(1, 4)),\n",
    "        'pred_scales': [[24], [48], [96], [192], [384]],\n",
    "        'pred_aspect_ratios': [ [[1, 1/2, 2]] ]*5,\n",
    "        'use_pixel_scales': True,\n",
    "        'preapply_sqrt': False,\n",
    "        'use_square_anchors': True,\n",
    "    }),\n",
    "    # 'backbone': resnet50_backbone.copy(),\n",
    "    # Update other training parameters\n",
    "    'num_classes': len(dataset_base.class_names) + 1,\n",
    "    'max_iter': 800,\n",
    "    'lr_steps': (500, 700),\n",
    "})\n",
    "\n",
    "\n",
    "# --- Activate Configuration ---\n",
    "cfg.replace(yolact_my_config)\n",
    "\n",
    "def set_cfg(config_name: Config):\n",
    "    global cfg\n",
    "    cfg.replace(config_name)\n",
    "\n",
    "set_cfg(yolact_my_config)\n",
    "\n",
    "print(\"Configuration is set for ResNet-50 backbone.\")\n",
    "print(f\"Number of classes: {cfg.num_classes}\")\n",
    "print(f\"Using CUDA: {USE_CUDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e053e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Initializing weights...\n",
      "Backbone weights loaded successfully.\n",
      "Begin training!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iteration >= cfg.max_iter:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/venv/myvenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/venv/myvenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/venv/myvenv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/venv/myvenv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Robotic/T5.2/ob2/yolact/data/coco.py:94\u001b[39m, in \u001b[36mCOCODetection.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m     87\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m        index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m \u001b[33;03m               target is the object returned by ``coco.loadAnns``.\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     im, gt, masks, h, w, num_crowds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpull_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m im, (gt, masks, num_crowds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Robotic/T5.2/ob2/yolact/data/coco.py:158\u001b[39m, in \u001b[36mCOCODetection.pull_item\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) > \u001b[32m0\u001b[39m:\n\u001b[32m    157\u001b[39m     target = np.array(target)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     img, masks, boxes, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_crowds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_crowds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# I stored num_crowds in labels so I didn't have to modify the entirety of augmentations\u001b[39;00m\n\u001b[32m    162\u001b[39m     num_crowds = labels[\u001b[33m'\u001b[39m\u001b[33mnum_crowds\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Robotic/T5.2/ob2/yolact/utils/augmentations.py:688\u001b[39m, in \u001b[36mSSDAugmentation.__call__\u001b[39m\u001b[34m(self, img, masks, boxes, labels)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, masks, boxes, labels):\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Robotic/T5.2/ob2/yolact/utils/augmentations.py:55\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img, masks, boxes, labels)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, masks=\u001b[38;5;28;01mNone\u001b[39;00m, boxes=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         img, masks, boxes, labels = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img, masks, boxes, labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Robotic/T5.2/ob2/yolact/utils/augmentations.py:309\u001b[39m, in \u001b[36mRandomSampleCrop.__call__\u001b[39m\u001b[34m(self, image, masks, boxes, labels)\u001b[39m\n\u001b[32m    306\u001b[39m height, width, _ = image.shape\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# randomly choose a mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     mode = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m image, masks, boxes, labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:956\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import datetime\n",
    "from data import COCODetection, detection_collate\n",
    "from utils.augmentations import SSDAugmentation\n",
    "\n",
    "def train():\n",
    "    # Setup\n",
    "    args = Args(use_cuda=USE_CUDA)\n",
    "    \n",
    "    if not os.path.exists(args.save_folder):\n",
    "        os.mkdir(args.save_folder)\n",
    "        \n",
    "    # Set device and tensor type\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "    # Dataset\n",
    "    dataset = COCODetection(image_path=cfg.dataset.train_images,\n",
    "                            info_file=cfg.dataset.train_info,\n",
    "                            transform=SSDAugmentation())\n",
    "\n",
    "    # Model\n",
    "    yolact_net = Yolact().to(device)\n",
    "    yolact_net.train()\n",
    "\n",
    "    print('Initializing weights...')\n",
    "    yolact_net.init_weights(backbone_path=args.save_folder + cfg.backbone.path)\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = optim.SGD(yolact_net.parameters(), lr=args.lr, momentum=args.momentum,\n",
    "                          weight_decay=args.decay)\n",
    "    criterion = MultiBoxLoss(num_classes=cfg.num_classes,\n",
    "                             pos_threshold=cfg.positive_iou_threshold,\n",
    "                             neg_threshold=cfg.negative_iou_threshold,\n",
    "                             negpos_ratio=cfg.ohem_negpos_ratio)\n",
    "\n",
    "    # Dataloader\n",
    "    data_loader = data.DataLoader(dataset, args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  shuffle=True, collate_fn=detection_collate,\n",
    "                                  pin_memory=False)\n",
    "\n",
    "    # Training Loop\n",
    "    iteration = 0\n",
    "    print('Begin training!')\n",
    "    for epoch in range(100): # Loop for a large number of epochs\n",
    "        if iteration >= cfg.max_iter:\n",
    "            break\n",
    "            \n",
    "        for datum in data_loader:\n",
    "            if iteration >= cfg.max_iter:\n",
    "                break\n",
    "\n",
    "            images, (targets, masks, num_crowds) = datum\n",
    "            images = torch.stack(images, 0).to(device)\n",
    "            targets = [ann.to(device) for ann in targets]\n",
    "            masks = [mask.to(device) for mask in masks]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = yolact_net(images)\n",
    "            losses = criterion(yolact_net, preds, targets, masks, num_crowds)\n",
    "            \n",
    "            losses = { k: v.mean() for k, v in losses.items() }\n",
    "            loss = sum([losses[k] for k in losses])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print(f'[{iteration:5d}] Loss: {loss.item():.4f}')\n",
    "\n",
    "            if iteration % args.save_interval == 0 and iteration != 0:\n",
    "                print(f'Saving state, iter: {iteration}')\n",
    "                yolact_net.save_weights(f'{args.save_folder}/yolact_custom_{iteration}.pth')\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    # Save final model\n",
    "    yolact_net.save_weights(f'{args.save_folder}/yolact_custom_final.pth')\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "# Start the training process\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
